# app_semseg_video.py
import os, cv2, time, tempfile, torch, gradio as gr
import numpy as np
import torch.nn.functional as F
from torchvision import transforms
from torchvision.models.segmentation import lraspp_mobilenet_v3_large
from PIL import Image, ImageDraw, ImageFont

# ---------------- CONFIG ----------------
CKPT_PATH  = "best_lraspp_mbv3.pth"   # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏ò checkpoint ‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå
CLASS_FILE = "_classes.csv"           # ‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡πÅ‡∏ö‡∏ö CSV: Pixel Value, Class
IMGSZ      = 640
ALPHA      = 0.45
DEVICE     = "cuda" if torch.cuda.is_available() else "cpu"
FORCE_NUM_CLASSES = None  # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™ ‡πÄ‡∏ä‡πà‡∏ô 63 ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡πÄ‡∏õ‡πá‡∏ô int
MIN_PIXELS_FOR_LABEL = 120  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô noise; ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏ñ‡∏∂‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏∂‡∏á‡∏à‡∏∞‡∏ß‡∏≤‡∏á‡∏õ‡πâ‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠
# ----------------------------------------


# ---------- Utils: ckpt & classes ----------
def _read_ckpt_state(ckpt_path):
    if not os.path.isfile(ckpt_path):
        raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö checkpoint: {ckpt_path}")
    ckpt = torch.load(ckpt_path, map_location="cpu")
    if not isinstance(ckpt, dict):
        raise RuntimeError("checkpoint ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô dict")
    sd = ckpt.get("model") or ckpt.get("state_dict") or ckpt
    sd = {k.replace("module.", ""): v for k, v in sd.items()}
    return sd

def _num_classes_from_state(sd):
    for k in ("classifier.low_classifier.bias",
              "classifier.low_classifier.weight",
              "classifier.high_classifier.bias",
              "classifier.high_classifier.weight"):
        if k in sd:
            return sd[k].shape[0]
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ö‡∏≠‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô checkpoint")

def load_classes(csv_path, require_n=None):
    names = []
    if os.path.isfile(csv_path):
        with open(csv_path, "r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if not s or s.lower().startswith("pixel"):  # header: Pixel Value, Class
                    continue
                parts = [p.strip() for p in s.split(",")]
                names.append(parts[-1])
    else:
        print("[WARN] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏•‡∏≤‡∏™ ‚Üí ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ['background','object']")
        names = ["background", "object"]

    if require_n is not None:
        if len(names) < require_n:
            print(f"[WARN] ‡πÄ‡∏û‡∏¥‡πà‡∏° placeholder ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö {require_n}")
            names += [f"class_{i}" for i in range(len(names), require_n)]
        elif len(names) > require_n:
            print(f"[WARN] ‡∏ï‡∏±‡∏î‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {require_n}")
            names = names[:require_n]
    return names

def build_palette(n):
    rng = np.random.default_rng(2025)
    palette = [(60,60,60)]  # background = ‡πÄ‡∏ó‡∏≤‡πÄ‡∏Ç‡πâ‡∏°
    for _ in range(max(0, n-1)):
        palette.append(tuple(int(x) for x in rng.integers(0,256,3)))
    return palette

def load_model_and_metadata(ckpt_path, class_file, force_n=None):
    sd = _read_ckpt_state(ckpt_path)
    ckpt_n = force_n or _num_classes_from_state(sd)
    model = lraspp_mobilenet_v3_large(weights=None, num_classes=ckpt_n)
    model.load_state_dict(sd, strict=True)
    model.to(DEVICE).eval()
    class_names = load_classes(class_file, require_n=ckpt_n)
    palette = build_palette(ckpt_n)
    return model, class_names, palette, ckpt_n

MODEL, CLASS_NAMES, PALETTE, NUM_CLASSES = load_model_and_metadata(
    CKPT_PATH, CLASS_FILE, force_n=FORCE_NUM_CLASSES
)


# ---------- Preprocess / color ----------
def preprocess(img_bgr):
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    t = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485,0.456,0.406],
                             std =[0.229,0.224,0.225])
    ])
    return t(img_rgb).unsqueeze(0)

def colorize_mask(mask):
    h, w = mask.shape
    color = np.zeros((h,w,3), dtype=np.uint8)
    for i,c in enumerate(PALETTE):
        color[mask==i] = c
    return color


# ---------- Label drawing (skip 'background') ----------
def _load_font(size_px):
    for p in [
        "C:/Windows/Fonts/THSarabunNew.ttf",
        "C:/Windows/Fonts/arial.ttf",
        "/System/Library/Fonts/Supplemental/Arial Unicode.ttf",
        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
    ]:
        if os.path.exists(p):
            try:
                return ImageFont.truetype(p, size_px)
            except Exception:
                pass
    try:
        return ImageFont.truetype("arial.ttf", size_px)
    except Exception:
        return ImageFont.load_default()

def add_class_labels(mask_color, pred_mask, class_names):
    """‡∏ß‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™ (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô background) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏î‡∏≥‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™"""
    img_pil = Image.fromarray(cv2.cvtColor(mask_color, cv2.COLOR_BGR2RGB)).convert("RGBA")
    draw = ImageDraw.Draw(img_pil)
    H, W = pred_mask.shape
    font = _load_font(max(16, min(H, W)//45))

    for cls_id, name in enumerate(class_names):
        if cls_id == 0:  # ‡πÑ‡∏°‡πà‡∏ß‡∏≤‡∏î background
            continue
        ys, xs = np.where(pred_mask == cls_id)
        if ys.size < MIN_PIXELS_FOR_LABEL:
            continue
        cy, cx = int(ys.mean()), int(xs.mean())
        text = name
        # ‡πÉ‡∏ä‡πâ textbbox ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°
        bbox = draw.textbbox((0, 0), text, font=font)
        tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]
        x1, y1 = max(0, cx - tw//2 - 5), max(0, cy - th//2 - 3)
        x2, y2 = min(W, x1 + tw + 10), min(H, y1 + th + 6)
        draw.rectangle([(x1, y1), (x2, y2)], fill=(0,0,0,180))
        draw.text(((x1+x2)//2, (y1+y2)//2), text, font=font, fill=(255,255,255,255), anchor="mm")

    return cv2.cvtColor(np.array(img_pil.convert("RGB")), cv2.COLOR_RGB2BGR)


# ---------- Core inference: image ----------
@torch.no_grad()
def predict_image(pil_image, alpha=ALPHA, draw_labels=True):
    if pil_image is None:
        return None, None
    img_bgr = np.array(pil_image)[..., ::-1]
    h, w = img_bgr.shape[:2]
    x = preprocess(cv2.resize(img_bgr, (IMGSZ, IMGSZ))).to(DEVICE)
    out = MODEL(x)["out"]
    out = F.interpolate(out, size=(h, w), mode="bilinear", align_corners=False)
    pred = out.argmax(1)[0].cpu().numpy().astype(np.uint8)

    mask_color = colorize_mask(pred)
    if draw_labels:
        mask_color = add_class_labels(mask_color, pred, CLASS_NAMES)
    overlay = cv2.addWeighted(img_bgr, 1.0, mask_color, alpha, 0)

    return cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB), cv2.cvtColor(mask_color, cv2.COLOR_BGR2RGB)


# ---------- Core inference: video (‡∏™‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î) ----------
@torch.no_grad()
def predict_video(video, alpha=ALPHA, stride=1, draw_labels=False, out_mode="overlay", progress=gr.Progress()):
    """
    ‡∏≠‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‚Üí ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡∏•‡∏∞‡πÄ‡∏ü‡∏£‡∏° ‚Üí ‡∏Ñ‡∏∑‡∏ô‡∏û‡∏≤‡∏ò‡πÑ‡∏ü‡∏•‡πå MP4
    out_mode: overlay / mask / side
    """
    # ‡πÅ‡∏õ‡∏•‡∏á input ‡∏Ç‡∏≠‡∏á Gradio ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô path ‡∏à‡∏£‡∏¥‡∏á
    vpath = None
    if isinstance(video, str) and os.path.exists(video):
        vpath = video
    elif isinstance(video, dict):
        for k in ("path", "name", "tempfile", "file"):
            p = video.get(k)
            if isinstance(p, str) and os.path.exists(p):
                vpath = p
                break
    if not vpath:
        return None

    cap = cv2.VideoCapture(vpath)
    if not cap.isOpened():
        return None

    fps = cap.get(cv2.CAP_PROP_FPS)
    fps = fps if fps and fps > 0 else 25.0
    W  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    H  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    N  = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT) else 0

    stride = max(1, int(stride))
    fps_out = max(1.0, fps / stride)

    # lazy writer: ‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏π‡πâ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏ü‡∏£‡∏°‡∏à‡∏£‡∏¥‡∏á
    tmpdir = tempfile.mkdtemp(prefix="semseg_")
    out_path = os.path.join(tmpdir, f"seg_out_{int(time.time())}.mp4")
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    vw = None

    frame_idx = 0
    total_iter = N if N > 0 else 1000000
    for _ in progress.tqdm(range(total_iter), desc="‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠"):
        ret, frame = cap.read()
        if not ret:
            break

        if stride > 1 and (frame_idx % stride != 0):
            frame_idx += 1
            continue

        # Inference ‡πÄ‡∏ü‡∏£‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        h, w = frame.shape[:2]
        x = preprocess(cv2.resize(frame, (IMGSZ, IMGSZ))).to(DEVICE)
        out = MODEL(x)["out"]
        out = F.interpolate(out, size=(h, w), mode="bilinear", align_corners=False)
        pred = out.argmax(1)[0].cpu().numpy().astype(np.uint8)

        mask_color = colorize_mask(pred)
        if draw_labels:
            mask_color = add_class_labels(mask_color, pred, CLASS_NAMES)

        if out_mode == "overlay":
            out_frame = cv2.addWeighted(frame, 1.0, mask_color, alpha, 0)
        elif out_mode == "mask":
            out_frame = mask_color
        else:  # side-by-side
            if mask_color.shape[:2] != frame.shape[:2]:
                mask_color = cv2.resize(mask_color, (w, h), interpolation=cv2.INTER_NEAREST)
            out_frame = cv2.hconcat([frame, mask_color])

        # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏ü‡∏£‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏π‡πà (‡∏ö‡∏≤‡∏á codec ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        oh, ow = out_frame.shape[:2]
        ow2 = ow - (ow % 2)
        oh2 = oh - (oh % 2)
        if ow2 != ow or oh2 != oh:
            out_frame = cv2.resize(out_frame, (ow2, oh2), interpolation=cv2.INTER_AREA)

        if vw is None:
            vw = cv2.VideoWriter(out_path, fourcc, fps_out, (out_frame.shape[1], out_frame.shape[0]))

        vw.write(out_frame)
        frame_idx += 1

        # safety stop ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏ï‡∏£‡∏µ‡∏°‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ü‡∏£‡∏°
        if N == 0 and frame_idx > 2000:
            break

    cap.release()
    if vw: vw.release()
    return out_path


# ---------- Gradio UI ----------
with gr.Blocks(title="Semantic Segmentation (Image + Video)") as demo:
    gr.Markdown("## üß† Semantic Segmentation ‚Äî LRASPP MobileNetV3\n‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢")

    # ---------- Tab: Image ----------
    with gr.Tab("‡∏†‡∏≤‡∏û‡∏ô‡∏¥‡πà‡∏á"):
        with gr.Row():
            inp_img = gr.Image(type="pil", label="‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û (JPG/PNG)")
            with gr.Column():
                out_overlay = gr.Image(type="numpy", label="‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
                out_mask    = gr.Image(type="numpy", label="Mask + ‡∏õ‡πâ‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠")

        with gr.Row():
            alpha_img = gr.Slider(0.0, 1.0, value=ALPHA, step=0.05, label="‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏∂‡∏ö Overlay")
            draw_lbl  = gr.Checkbox(value=True, label="‡πÅ‡∏õ‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏ö‡∏ô Mask (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô background)")

        btn_img = gr.Button("‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏†‡∏≤‡∏û")
        btn_img.click(fn=predict_image, inputs=[inp_img, alpha_img, draw_lbl], outputs=[out_overlay, out_mask])

    # ---------- Tab: Video ----------
    with gr.Tab("‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠"):
        with gr.Row():
            inp_vid = gr.Video(label="‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠", sources=["upload"], interactive=True)
            out_vid = gr.Video(label="‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (MP4)", autoplay=True, show_download_button=True)

        with gr.Row():
            alpha_vid  = gr.Slider(0.0, 1.0, value=ALPHA, step=0.05, label="‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏∂‡∏ö Overlay")
            stride_vid = gr.Slider(1, 8, value=1, step=1, label="‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏ü‡∏£‡∏° (‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß)")
            draw_lbl_v = gr.Checkbox(value=False, label="‡πÅ‡∏õ‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏ö‡∏ô‡πÄ‡∏ü‡∏£‡∏° (‡∏ä‡πâ‡∏≤‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)")
            out_mode_v = gr.Radio(
                choices=[("Overlay","overlay"), ("Mask-only","mask"), ("Side-by-side","side")],
                value="overlay",
                label="‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏≠‡∏≤‡∏ï‡πå‡∏û‡∏∏‡∏ï‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠"
            )

        btn_vid = gr.Button("‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠")
        btn_vid.click(
            fn=predict_video,
            inputs=[inp_vid, alpha_vid, stride_vid, draw_lbl_v, out_mode_v],
            outputs=[out_vid],
            queue=True
        )

    gr.Markdown("### üè∑Ô∏è ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™ (‡∏ï‡∏≤‡∏° checkpoint)")
    gr.Dataframe(headers=["ID","Class Name"], value=[[i,n] for i,n in enumerate(CLASS_NAMES)], interactive=False)

    with gr.Accordion("‚ÑπÔ∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö", open=False):
        gr.Markdown(f"- **Device**: `{DEVICE}`\n- **Classes (from ckpt)**: `{NUM_CLASSES}`")

# ‡πÄ‡∏õ‡∏¥‡∏î share=True ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Colab/‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å
if __name__ == "__main__":
    demo.queue().launch(share=True, debug=True)
